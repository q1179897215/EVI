_target_: src.models.new.NewMiLitModel

# To avoiding copying of loss and metric names, during hydra initialization
# there is custom resolver which replaces __loss__ to loss.__class__.__name__
# and __metric__ to main_metric.__class__.__name__,
# for example: ${replace:"__metric__/valid"}
# Use quotes for defining internal value in ${replace:"..."} to avoid
# grammar problems with hydra config parser.

model:
  _target_: src.models.new.New
  embedding_layer:
    _target_: src.models.common.AlldataEmbeddingLayer
    batch_type: ${data.batch_type}
    embedding_size: 5
  task_num: 3
  expert_num: 8
  expert_dims: [256]
  expert_dropout: [0.3]
  tower_dims: [128, 64, 32]
  tower_dropout: [0, 0, 0]


loss:
  _target_: src.models.new.CvrAllSpaceMultiTaskLoss
  ctr_loss_proportion: 1.0
  cvr_loss_proportion: 1.0
  ctcvr_loss_proportion: 0.2
  unclick_space_loss_proportion: 0.2

lr: 0.001

weight_decay: 1.0e-6

batch_type: ${data.batch_type}

mi_ratio: 0.8
var_ratio: 5.0
info_layer_num: 0.0
# embedding:

#     embedding_size = 5,
#     weight_decay = 1e-6,
#     presion = '32',
#     expert_num = 8,
#     task_num = 3,
#     embed_output_dim = 90,
#     bottom_mlp_dims = [256],
#     tower_mlp_dims = [128, 64, 32],
#     dropout_expert = [0.3],
#     dropout_tower = [0.1, 0.3, 0.3],
#     batch_type = 'fr',

# scheduler:
#   scheduler:
#     _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
#     mode: "max"
#     factor: 0.3
#     min_lr: 1.0e-9
#     patience: 10
#     verbose: True
#   extras:
#     monitor: ${replace:"__metric__/valid"}
#     interval: "epoch"
#     frequency: 1

# logging:
#   on_step: False
#   on_epoch: True
#   sync_dist: True
#   prog_bar: True
